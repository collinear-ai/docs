---
title: Creating a Reliability Evaluation
description: Use the Collinear AI Platform to create a new reliability evaluation.
icon: circle-check
---
## ✅ What is a Reliability Evaluation?

A **Reliability Evaluation** measures how consistently and truthfully your model responds across a dataset. Collinear AI runs each sample through a selected **reliability judge**, which detects hallucinations or factual inconsistencies.

This helps you:

- Quantify your model’s factual accuracy
- Identify hallucination-prone outputs
- Compare performance across different models or prompts


## 🎥 Interactive Walkthrough

Want to see it in action? Follow this guided demo to create your reliability run:

<div style={{ position: 'relative', paddingBottom: 'calc(56.67989417989418% + 41px)', height: 0, width: '100%' }}>
      <iframe
        src="https://demo.arcade.software/al0t0gUv3LWe9YAxnk5S?embed&embed_mobile=tab&embed_desktop=inline&show_copy_link=true"
        title="Streamline AI Model Evaluations with Collinear"
        frameBorder="0"
        loading="lazy"
        allowFullScreen
        allow="clipboard-write"
        style={{ position: 'absolute', top: 0, left: 0, width: '100%', height: '100%', colorScheme: 'light' }}
      />
    </div>


# Introduction

Once you connect your model or upload your dataset, you can run a reliability evaluation on it using Collinear AI's suite of reliability judges.

---

## 🚀 Getting Started

After connecting your model or uploading your dataset, you can initiate a **reliability evaluation** using one of Collinear AI’s reliability judges.

---

## 🧑‍⚖️ Select a Judge

Choose from the following reliability models:

1. **Lynx 8B** – Patronus AI’s off-the-shelf model for hallucination detection.
2. **Veritas Nano** – Collinear’s ultra-fast binary classifier for hallucination detection.
3. **Veritas** – Collinear’s advanced large model for in-depth hallucination detection.
4. **Prompted Model** – Use any custom model with a tailored prompt for flexible evaluation.

---

## 🧠 Select a Context Engine

Choose how you'd like to include contextual grounding during evaluation:

### Options

1. **Use Context From Dataset**
   Pulls relevant context directly from your uploaded dataset.

2. **Add Context Engine**
   Use a RAG (Retrieve-and-Generate) engine to provide additional context.

#### Required Fields for RAG Integration:

- **Content Engine API Key** – Authenticate securely with your context engine.
- **RAG Host** – URL for the server powering the RAG service.
- **Index** – Optimized data structure for efficient search and retrieval.
- **Namespace** – Logical grouping to avoid identifier conflicts.
- **Top K** – Controls how many of the top results to fetch from the index.

---
