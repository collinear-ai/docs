---
title: Create A Safety Judge
description: Use the Collinear AI Platform to create a safety judge for your space.
---
Use the Collinear AI Platform to create a safety judge for your space. 

Safety Judges help keep language models safe by filtering out harmful content. They ensure that responses are:
- legal
- respectful 
- safe for users

# Steps to Create a Safety Judge

## Select “safety judge”
<img
    style={{borderRadius: '0.5rem'}}
    src="/images/ssafetyjudge.png"
/>

 ## Select Select Safety Model type 
You can choose between:
- Llama Guard
- Wild Guard
- Collinear Guard
- Collinear Guard Nano
- Prompted Model
<img
    style={{borderRadius: '0.5rem'}}
    src="/images/selectsafetyjudge.png"
/>

## 1. Creating LLama Guard judge 
This LLama Guard judge provides a simple and direct safety assessment, ensuring that unsafe content is flagged and only safe content passes through.
LLamaGuard Evaluation: Binary classification {0, 1}
0: The content is deemed unsafe.
1: The content is considered safe.

Once you select Llama Guard judge, select "Continue" 
<img
    style={{borderRadius: '0.5rem'}}
    src="/images/selectsllamaguard.png"
/>

##Set Judge Name 
Name it according to your preference and select "Create Judge"
<img
    style={{borderRadius: '0.5rem'}}
    src="/images/createajudgebutton.png"
/>

##2. Creating Wild Guard judge 
The Wild Guard judge provides a straightforward safety evaluation for prompts and responses, along with refusal handling, ensuring that unsafe interactions are flagged and refusals are properly identified.
Prompt Evaluation: Binary classification {0, 1}

0: The prompt is deemed unsafe.
1: The prompt is considered safe.
Response Evaluation: Binary classification {0, 1}

0: The response is deemed unsafe.
1: The response is considered safe.
Refusal Evaluation: Binary classification {0, 1}

0: Indicates the model refused to generate a response.
1: Indicates the model successfully generated a response.

Once you select Llama Guard judge, select "Continue" 
<img
    style={{borderRadius: '0.5rem'}}
    src="/images/wildguard.png"
/>

## Set Judge Name
Name it according to your preference and select "Create Judge"
<img
    style={{borderRadius: '0.5rem'}}
    src="/images/createajudgebutton.png"
/>

##3. Creating Collinear Guard judge
This judge evaluates the safety of model outputs on a more granular scale, providing a detailed assessment of the content's safety level. The Likert scale enables a nuanced view, from identifying highly unsafe outputs to confirming very safe responses.
Safety Rating: Likert scale rating from 1 to 5
1: Very unsafe
2: Unsafe
3: Neutral
4: Safe
5: Very safe
This judge evaluates the safety of model outputs on a more granular scale, providing a detailed assessment of the content's safety level. The Likert scale enables a nuanced view, from identifying highly unsafe outputs to confirming very safe responses.

This version provides clarity and emphasizes the value of a more detailed safety assessment using the Likert scale. Let me know if you'd like any changes!

Once you select Collinear Guard judge, select "Continue" 
<img
    style={{borderRadius: '0.5rem'}}
    src="/images/collinearguard.png"
/>

##Set Judge Name
Name it according to your preference and select "Create Judge"
<img
    style={{borderRadius: '0.5rem'}}
    src="/images/createajudgebutton.png"
/>

##4. Creating Collinear Guard Nano judge 
This judge is designed to assess both prompts and responses for safety, while also determining whether the model has appropriately refused to generate a response based on safety or other criteria.
Prompt Evaluation: Binary classification {0, 1}

0: The prompt is deemed unsafe.
1: The prompt is considered safe.
Response Evaluation: Binary classification {0, 1}

0: The response is deemed unsafe.
1: The response is considered safe.
Refusal Evaluation: Binary classification {0, 1}

0: Indicates the model refused to generate a response.
1: Indicates the model successfully generated a response.

Once you select Collinear Guard Nano judge, select "Continue" 
<img
    style={{borderRadius: '0.5rem'}}
    src="/images/collinearguardnanoo.png"
/>

##Select your Model Type 
Choose Response, Prompt and Refusal and select "Continue"
<img
    style={{borderRadius: '0.5rem'}}
    src="/images/selectyourmodelnano.png"
/>

##Set Judge Name 
Name it according to your preference and select "Create Judge"
<img
    style={{borderRadius: '0.5rem'}}
    src="/images/createajudgebutton.png"
/>

##5. Creating Prompted Model judge
This safety judge will evaluate model outputs based on predefined safety criteria, ensuring that unsafe responses are flagged for further review, while safe outputs are approved for deployment.
Output: Binary classification {0, 1}
0: Indicates the response is deemed unsafe.
1: Indicates the response is considered safe.

Once you select Prompted Model judge, select "Continue"
<img
    style={{borderRadius: '0.5rem'}}
    src="/images/promptedmodel.png"
/>

##Select your Prompted Model
You can select your model from the drop down, if you haven't added a model select "Add New Model" to create a new model
<img
    style={{borderRadius: '0.5rem'}}
    src="/images/selectyourmodelprompted.png"
/>

<img
    style={{borderRadius: '0.5rem'}}
    src="/images/addanewmodelprompted.png"
/>

##Edit your prompt template 
You can proceed with the template or edit it and then select "Continue"
<img
    style={{borderRadius: '0.5rem'}}
    src="/images/prompttemplate.png"
/>

##Set Judge Name 
Name it according to your preference and select "Create Judge"
<img
    style={{borderRadius: '0.5rem'}}
    src="/images/createajudgebutton.png"
/>




